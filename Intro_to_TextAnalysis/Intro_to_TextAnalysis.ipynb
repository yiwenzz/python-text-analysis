{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's workshop will address concepts in text analysis. A fundamental understanding of Python is necessary. We will cover:\n",
    "\n",
    "1. term-document model\n",
    "2. regex\n",
    "3. POS tagging\n",
    "3. sentiment analysis\n",
    "4. topic modeling\n",
    "5. word2vec\n",
    "\n",
    "Python packages you will need:\n",
    "\n",
    "* NLTK ( `$ pip install nltk` )\n",
    "* TextBlob ( `$ pip install textblob` )\n",
    "* gensim ( `$ pip install gensim` )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We've spent a lot of time in Python dealing with text data, and that's because text data is everywhere. It is the primary form of communication between persons and persons, persons and computers, and computers and computers. The kind of inferential methods that we apply to text data, however, are different from those applied to tabular data. \n",
    "\n",
    "This is partly because documents are typically specified in a way that expresses both structure and content using text (i.e. the document object model).\n",
    "\n",
    "Largely, however, it's because text is difficult to turn into numbers in a way that preserves the information in the document. Today, we'll talk about dominant language models in NLP and the basics of how to implement it in Python.\n",
    "\n",
    "# Part 1: The term-document model and preprocessing text\n",
    "\n",
    "The term-document model is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored.\n",
    "\n",
    "This is the primary method still used for most text analysis, although models utilizing word embeddings are beginning to take hold. We will discuss word embeddings briefly at the end.\n",
    "\n",
    "In order to actually turn our text into a bag of words, we'll have to do some preprocessing. This is a crucial step at the beginning of any NLP project, and much of this first section will involve it.\n",
    "\n",
    "To start with, let's import NLTK and load a document from their toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('webtext')\n",
    "document = nltk.corpus.webtext.open('grail.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've gotten ourselves a bit of the script from *Monty Python and the Holy Grail*. Note that when we are looking at the text, part of the structure of the document is written in tokens. For example, stage directions have been placed in brackets, and the names of the person speaking are in all caps.\n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "If we wanted to read out all of the stage directions for analysis, or just King Arthur's lines, doing so in base Python string processing will be very difficult. Instead, we are going to use regular expressions. Regular expressions are a method for string manipulation that match patterns instead of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "snippet = document.split(\"\\n\")[8]\n",
    "print(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use regex to see if 'coconuts' is in `snippet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'coconuts', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is! As you see, it gives us the indices, which we can also get using the `span` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = re.search(r'coconuts', snippet).span()\n",
    "print(indices)\n",
    "\n",
    "print(snippet[indices[0]:indices[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with `str.find`, we can search for plain text. But `re` also gives us the option for searching for patterns of bytes - like only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'[a-z]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've told `re` to search for the first sequence of bytes that is only composed of lowercase letters between `a` and `z`. We could get the letters at the end of each sentence by including a bang at the end of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'[a-z]!', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things happening here:\n",
    "\n",
    "1. `[` and `]` do not mean 'bracket'; they are special characters which mean 'anything of this class'\n",
    "2. we've only matched one letter each\n",
    "\n",
    "Re is flexible about how you specify numbers - you can match none, some, a range, or all repetitions of a sequence or character class.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`{x}`     | exactly x repetitions\n",
    "`{x,y}`   | between x and y repetitions\n",
    "`?`       | 0 or 1 repetition\n",
    "`*`       | 0 or many repetitions\n",
    "`+`       | 1 or many repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the power of regular expressions are their special characters. Common ones that you'll see are:\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`.`       | match anything except a newline\n",
    "`^`       | match the start of a line\n",
    "`$`       | match the end of a line\n",
    "`\\s`      | matches any whitespace or newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to grab all of Arthur's speech without grabbing the name `ARTHUR` itself?\n",
    "\n",
    "If we wanted to do this using base string manipulation, we would need to do something like:\n",
    "\n",
    "```\n",
    "split the document into lines\n",
    "create a new list of just lines that start with ARTHUR\n",
    "create a newer list with ARTHUR removed from the front of each element\n",
    "```\n",
    "\n",
    "Regex gives us a way of doing this in one line, by using something called groups. Groups are pieces of a pattern that can be ignored, negated, or given names for later retrieval.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`(x)`     | match x\n",
    "`(?:x)`   | match x but don't capture it\n",
    "`(?P<x>)` | match something and give it name x\n",
    "`(?=x)`   | match only if string is followed by x\n",
    "`(?!x)`   | match only if string is not followed by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'(?:ARTHUR: )(.+)', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this regex down.\n",
    "\n",
    "The first group `(?:ARTHUR: )` will match but not capture all instances of the string \"ARTHUR: \". All of Arthur's lines start with this string, but we only want what follows, so this regex allows us to filter out his name.\n",
    "\n",
    "The second group `(.+)` will match anything except a newline, for 1 or many repetitions. That means this will capture whatever follows the string \"ARTHUR: \" in that same line.\n",
    "\n",
    "Because we are using `findall`, the regex engine is capturing and returning the normal groups, but not the non-capturing group. For complicated, multi-piece regular expressions, you may need to pull groups out separately. You can do this with regex names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'(?P<name>[A-Z ]+)(?:: )(?P<line>.+)')\n",
    "match = re.search(p, document)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name `(?P<name>[A-Z ]+)`, which we are coincidentally calling \"name\", will search for strings consisting of upper case characters and white space.\n",
    "\n",
    "The second group `(?:: )` will match \": \" but not capture it.\n",
    "\n",
    "The third group `(?P<line>.+)` is another name, which we are calling \"line\". This will simply get all the characters after the second group, up to a newline.\n",
    "\n",
    "To get our names, we just call the `group` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match.group('name'))\n",
    "print(match.group('line'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Regex parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the regex pattern `p` above to print the `set` of unique character names in *Monty Python*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have 84 different characters.\n",
    "\n",
    "Now use the `set` you made above to gather all dialogue into a dictionary called `char_dict`, with the keys being the character name and the value being a list of that character's lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_dict[\"ARTHUR\"] should give you a list of strings with his dialogue\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict[\"ARTHUR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Let's grab Arthur's speech from above, and see what we can learn about Arthur from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur = ' '.join(char_dict[\"ARTHUR\"])\n",
    "snippet = arthur[1000:1100]\n",
    "print(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model for natural language, we're interested in words. The document is currently a continuous string of bytes, which isn't ideal.\n",
    "\n",
    "The practice of pulling apart a continuous string into units is called \"tokenizing\", and it creates \"tokens\". NLTK, the canonical library for NLP in Python, has a couple of implementations for tokenizing a string into sentences, and sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "word_tokenize(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what happened to \"didn't\". It's been separated into \"did\" and \"n't\", which keeps with the way contractions work in English. While we know we could just use `snippet.split()` to split on white space, or write a complicated regex, word tokenizers allow for a more accurate representation of words based on additional rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice word tokenizers also separate punctuation, so unlike if we had split on whitespace, word tokenizers won't end up with `there!` and `there` as being different words.\n",
    "\n",
    "At this point, we can start asking questions like what are the most common words, how many are unqiue words, and what words tend to occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(arthur)\n",
    "len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see right away that Arthur is using the same words a whole bunch - on average, each unique word is used four times. This is typical of natural language. \n",
    "\n",
    "> Not necessarily the value, but that the number of unique words in any corpus increases much more slowly than the total number of words.\n",
    "\n",
    "> A corpus with 100M tokens, for example, probably only has 100,000 unique tokens in it.\n",
    "\n",
    "For more complicated metrics, it's easier to use NLTK's classes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import collocations\n",
    "fd = collocations.FreqDist(tokens)\n",
    "fd.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so interesting, a common step in text analysis is to remove noise. *However*, what you deem \"noise\" is not only very important but also dependent on the project at hand. For the purposes of today, we will discuss two common categories of strings often considered \"noise\". \n",
    "\n",
    "- Punctuation: While important for sentence analysis, punctuation will get in the way of word frequency and n-gram analyses. They will also affect any clustering on topic modeling.\n",
    "\n",
    "- Stopwords: Stopwords are the most frequent words in any given language. Words like \"the\", \"a\", \"that\", etc. are considered not semantically important, and would also skew any frequency or n-gram analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function below that takes a string as an argument and returns a list of words without punctuation or stopwords.\n",
    "\n",
    "`punctuation` is a list of punctuation strings, and we have created the list `stop_words` for you.\n",
    "\n",
    "Hint: first you'll want to remove punctuation, then tokenize, then remove stop words. Make sure you account for upper and lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_punc_stop(text_string):\n",
    "    \n",
    "    from string import punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rerun our frequency analysis without the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_reduced = rem_punc_stop(arthur)\n",
    "fd2 = collocations.FreqDist(tokens_reduced)\n",
    "fd2.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at collocations. In NLTK these can be calculated by either pointwise mutual information or likelihood ratio. More on those measures [here](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pmi\n",
    "measures = collocations.BigramAssocMeasures()\n",
    "c = collocations.BigramCollocationFinder.from_words(tokens_reduced)\n",
    "c.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#likelihood\n",
    "c.nbest(measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the collocation finder is pulling out some things that have face validity. When Arthur is talking about peasants, he calls them \"bloody\" more often than not. However, collocations like \"Brother Maynard\" and \"BLACK KNIGHT\" are less informative to us, because we know that they are proper names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many applications require text to be in the form of a list of sentences. NLTK's `sent_tokenize` should do the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(arthur)\n",
    "sents[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common step in the NLP pipeline is tagging for part of speech, which can help begin to rectify our \"bag of words\" approach by retaining some idea of syntax. While training a POS tagger is a workshop in itself, NLTK also provides a trained tagger for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk import pos_tag\n",
    "\n",
    "toks_and_sents = [word_tokenize(s) for s in sent_tokenize(arthur)]\n",
    "tagged_sents = [pos_tag(s) for s in toks_and_sents]\n",
    "\n",
    "print()\n",
    "print(tagged_sents[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the POS tagset NLTK uses, see [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: POS Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a frequency distribution for Arthur's parts of speech using the `nltk.FreqDist()` method.  You'll need to first create a list of just the POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tags)\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing\n",
    "\n",
    "In NLP it is often the case that the specific form of a word is not as important as the idea to which it refers. For example, if you are trying to identify the topic of a document, counting 'running', 'runs', 'ran', and 'run' as four separate words is not useful. Reducing words to their stems is a process called stemming.\n",
    "\n",
    "A popular stemming implementation is the Snowball Stemmer, which is based on the [Porter Stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html). Its algorithm looks at word forms and does things like drop final 's's, 'ed's, and 'ing's.\n",
    "\n",
    "Just like the tokenizers, we first have to create a stemmer object with the language we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = nltk.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try stemming some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball.stem('embarassed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball is a very fast algorithm, but it has a lot of edge cases. In some cases, words with the same stem are reduced to two different stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball.stem('cylinder'), snowball.stem('cylindrical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, two different words are reduced to the same stem.\n",
    "\n",
    "> This is sometimes referred to as a 'collision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball.stem('vacation'), snowball.stem('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more accurate approach is to use an English word bank like WordNet to call dictionary lookups on word forms, in a process called lemmatization.\n",
    "\n",
    "Whereas stemming just algorithmically cuts off the ends of words, lemmatization takes into account the grammatical and morphological properties of the word. More on the two [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "wordnet = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.lemmatize('vacation'), wordnet.lemmatize('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most common lemmata in our tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_red_lem = [snowball.stem(w) for w in tokens_reduced]\n",
    "fd3 = collocations.FreqDist(tok_red_lem)\n",
    "fd3.most_common()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: High-level analysis\n",
    "\n",
    "The rest of this class will focus on high level analyses, which do most of what we just covered for you, or in one quick step. It is important to remember that it is performing the above first. To know how to correctly interpret your analysis, remember that at some point the computer decided certain things weren't important!\n",
    "\n",
    "## Sentiment\n",
    "\n",
    "Frequently, we are interested in text to learn something about the person who is speaking. One of these things we've talked about already - linguistic diversity. A similar metric was used a couple of years ago to settle the question of who has the [largest vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html).\n",
    "\n",
    "> Unsurprisingly, top spots go to Canibus, Aesop Rock, and the Wu Tang Clan. E-40 is also in the top 20, but mostly because he makes up a lot of words; as are OutKast, who print their lyrics with words slurred in the actual typography\n",
    "\n",
    "Another thing we can learn is about how the speaker is feeling, with a process called sentiment analysis. Before we start, be forewarned that this is not a robust method by any stretch of the imagination. Sentiment classifiers are often trained on product reviews, which limits their ecological validity.\n",
    "\n",
    "We're going to use TextBlob because it's an easy way to work with text data, and has a built in sentiment classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(arthur)\n",
    "blob.sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the polarity of a string, we can just iterate through Arthur's sentences. TextBlob will calculate the polarity of each sentence with `sentiment.polarity`, and we can just add it to our accumulator variable `net_pol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pol = 0\n",
    "for sentence in blob.sentences:\n",
    "    pol = sentence.sentiment.polarity\n",
    "    print(pol, sentence)\n",
    "    net_pol += pol\n",
    "print()\n",
    "print(\"Net polarity of Arthur: \", net_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening behind the scenes? While there are new algorithms for sentiment anaysis emerging (cf. `VADER`), most algorithms currently rely only on a `dictionary` of words and a corresponding `positive`, `negative`, or `neutral`. Based on all the words in a sentence, a value is calculated for the sentence as a whole. Not super fancy, I know. Of course, you can change the `dictionary` used in the library itself, or opt for more advanced algorithms that aim to capture context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we look at all characters? Create an empty list `collected_stats` and iterate through `char_dict`, calculate the net polarity of each character, and append a tuple of e.g. `(ARTHUR, 11.45)` back to `collected_stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_stats = []\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `sort` this list of tuples by polarity, and print the list of characters in *Monty Python* according to their sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Another common NLP task is topic modeling. The math behind this is beyond the scope of this course, but the basic strategy is to represent each document as a one-dimensional array, where the indices correspond to integer ids of tokens in the document. Then, some measure of semantic similarity, like the cosine of the angle between unitized versions of the document vectors, is calculated. Finally, distinct topics are identified as leading certain groups of documents. The result is a list of `n` topics with the driving words for that topic, and a list of documents with their relation to each topic (how strongly a document fits that topic.\n",
    "\n",
    "Let's run a topic model on the characters of *Monty Python*.\n",
    "\n",
    "Luckily for us there is another Python library that takes care of the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to separate the speeches and people, but keep it ordered so we index correctly when done. For the speeches, we'll need all speech as one string, then tokenized. We also need to remove punctuation and stop words so that Python can identify important words to documents. It seems we've gotten lucky again, we already wrote *rem_punc_stop*! Finally we'll stem our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "speeches = []\n",
    "for k,v in char_dict.items():\n",
    "    people.append(k)\n",
    "    new_string = ' '.join(v)  # join all dialogue pices\n",
    "    toks = rem_punc_stop(new_string)  # remove punctuation and stop words, and tokenize\n",
    "    stems = [snowball.stem(tok) for tok in toks]  # change words to stems\n",
    "    speeches.append(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a gensim dictionary which will map the words in our speeches to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(speeches)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will filter out words at the extremes. The `no_below` argument refers to the absolute number of documents in which a word occurs, and the `no_above` argument is a fraction of the corpus.\n",
    "\n",
    "Since we want to create topics around which to cluster texts, we don't want to use words that only very rarely occur, since they won't be relevant to many texts. Similarly, if a word appears in too many texts, it's probably not a very useful identifier for a subgroup of the texts, so we don't want to have it in a topic either.\n",
    "\n",
    "As you can imagine, in your own work you'll want to try different values to see what's best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=2, no_above=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our bag of words corpus with the `doc2bow` method. Each text is now represented as a list of tuples, in which the first item is an integer corresponding to a word, and the second item is its frequency in that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(i) for i in speeches]\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set the parameters for the [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) topic modelling (other algorithms such as [LSI](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing) do exist, but we won't get into the differences today):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we run chunks of 15 texts, and update after every 2 chunks, and make 10 passes\n",
    "lda = models.LdaModel(corpus, num_topics=6, \n",
    "                            update_every=2,\n",
    "                            id2word=dictionary, \n",
    "                            chunksize=15, \n",
    "                            passes=10)\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `get_document_topics` method to get the topics for a given document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(people[4])\n",
    "lda.get_document_topics(corpus[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate through our corpus and find the best matching topic for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(corpus):\n",
    "    \n",
    "    topic, score = max(lda.get_document_topics(v), key = lambda x:x[1])\n",
    "    print(\"Character: \" + people[i])\n",
    "    print()\n",
    "    print(\"Highest topic score: \" + str(score))\n",
    "    print()\n",
    "    print(\"Topic: \" + str(lda.show_topic(topic)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are the first successful attempt to move away from the \"bag of words\" model of language. Instead of looking at word frequencies, and vocabulary usage, word embeddings aim to retain syntactic information. Generally, a word2vec model *will not* remove stopwords or punctuation, because they are vital to the model itself.\n",
    "\n",
    "word2vec simply changes a tokenized sentence into a vector of numbers, with each unique token being its own number.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "~~~\n",
    "[[\"I\", \"like\", \"coffee\", \".\"], [\"I\", \"like\", \"my\", \"coffee\", \"without\", \"sugar\", \".\"]]\n",
    "~~~\n",
    "\n",
    "is tranformed to:\n",
    "\n",
    "~~~\n",
    "[[43, 75, 435, 98], [43, 75, 10, 435, 31, 217, 98]]\n",
    "~~~\n",
    "\n",
    "Notice, the \"I\"s, the \"likes\", the \"coffees\", and the \".\"s, all have the same assignment.\n",
    "\n",
    "The model is created by taking these numbers, and creating a high dimensional vector by mapping every word to its surrounding, creating a sort of \"cloud\" of words, where words used in a similar syntactic, and often semantic, fashion, will cluster closer together.\n",
    "\n",
    "One of the drawbacks of word2vec is the volume of data necessary for a decent analysis. So we will read in a copy of the King James Bible and hope it will provide enough data, it then needs to be broken into sentences and tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"King_James_Bible.txt\", \"r\") as f:\n",
    "    bible = f.read()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "bible = sent_tokenize(bible)\n",
    "bible = [word_tokenize(s) for s in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually train the model on the language of the Bible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.word2vec.Word2Vec(bible, size=300, window=5, min_count=5, workers=4)\n",
    "model.train(bible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can look at how words are situated in this cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create little equations, so what would be a:\n",
    "\n",
    "KING + WOMAN - MAN = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: word2vec\n",
    "\n",
    "Play around with the word2vec model above and try to put into words exactly what the model does, and how one should interpret the results. How would you contrast this with the \"bag of words\" model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
